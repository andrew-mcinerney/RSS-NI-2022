---
title: "Feedforward Neural Networks as Statistical Models"
author: "Andrew McInerney"
coauthor: "Kevin Burke"
institution: "University of Limerick"
event: "RSS Northern Ireland"
date: '26 Oct 2022'
output:
  xaringan::moon_reader:
    self_contained: true
    css: [css/default.css, css/fonts.css]
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: '16:9'
header-includes:
  - \usepackage{booktabs}
editor_options: 
  chunk_output_type: console
---

```{r, echo=FALSE, message=FALSE}
library(knitr)
library(fontawesome)
```


```{r analysis, include=FALSE, cache=TRUE}

# load packages -----------------------------------------------------------
library(ISLR2)
library(tidyverse)
library(selectnn)

# load data ---------------------------------------------------------------
data(Boston)

df <- Boston

# split into traning and test ---------------------------------------------
set.seed(6464371)
forTraining <- caret::createDataPartition(df$medv,
                                          p = 0.8)[[1]]
trainingSet <- df[forTraining,]
testSet <- df[-forTraining,]


# prep data --------------------------------------------------------

std <- caret::preProcess(trainingSet, method = 'range')
X_train <- as.matrix(predict(std, trainingSet))[, -ncol(df)]
X_test <- as.matrix(predict(std, testSet))[, -ncol(df)]

y_train <- as.matrix(predict(std, trainingSet))[, ncol(df)]
y_test <- as.matrix(predict(std, testSet))[, ncol(df)]

Boston = as.data.frame(cbind(X_train, y_train))

colnames(Boston)[13] <- 'medv'

Boston <- Boston[, -c(10)]

nn <- selectnn(medv ~ ., data = Boston, Q = 10, n_init = 10, maxit = 5000)

X <- nn$X
nn_nnet <- nnet::nnet(X, Boston$medv, Wts = nn$W_opt, size = nn$q,
                      trace = FALSE, linout = TRUE, maxit = 0) 

library(statnnet)
stnn <- statnnet(nn_nnet, X)  
summary(stnn)

```

class: title-slide, left, bottom

# `r rmarkdown::metadata$title`
----
## **`r rmarkdown::metadata$author`**, **`r rmarkdown::metadata$coauthor`**
### `r rmarkdown::metadata$institution`
#### `r rmarkdown::metadata$event`, `r rmarkdown::metadata$date`

???

Intro.
I am funded by SFI CRT in FDS.
This is a multi-institutional collaboration between UL, UCD and MU.
The goal of the centre is to fuse and blend the fundamentals of applied mathematics
machine learning, and statistics.
My research focuses on the combination of the latter two.
So, neural networks are typically implemented as black-box models in machine
learning, but taking a statistical perspective, I want to show how these models
have similarities to the models traditionally used in statistical model.

---

# Agenda


--

- Feedforward Neural Networks

???
Brief intro. to the history of NNs and the early work by statisticians in the
field.
Then I will introduce the FNN model.

--

- Statistical Perspective

--

- Model Selection

--

- Statistical Interpretation

--
 
- R Implementation

--

<br>

Slides: [bit.ly/rss-fnn-stat](https://bit.ly/rss-fnn-stat)   Code: [bit.ly/rss-fnn-stat-code](https://bit.ly/rss-fnn-stat-code)

---
class: inverse middle center subsection
# Feedforward Neural Networks

---

# Background

--

Neural networks originated from attempts to model the human brain.

<br>

--

Early influential papers:

--

-  McCulloch and Pitts (1943)

--

-  Rosenblatt (1958)

--

-  Rumelhart, Hinton and Williams (1986)



---

# Background

Interest within the statistics community in the late 1980s and early 1990s.

--

<br>

Comprehensive reviews provided by White (1989), Ripley (1993), Cheng and Titterington (1994).

--

<br>

However, majority of research took place outside the field of statistics
(Breiman, 2001; Hooker and Mentch, 2021).

???
In the late 1980s to early 1990s, there was an interest within the statistics
in neural networks. 
Some really comprehensive reviews of neural networks from a statistical
perspective are given by White ..., Ripley, ..., and Cheng and Titterington ...
However, since then, neural network research has been primarily conducted outside
the field of statistics.
It has primarily been conducted by computer scientists, and now machine learning
researchers.
But, there have been calls to try an align this research with statistics.
And the potential benefit is two fold. 
NNs are more flexible than traditional nonlinear regression models.
Also, a statistical-modelling perspective can help improve the implementation,
and, more importantly, the interpretation of these models.
Examples of this can be found in Rugamer and ..., which drawn on NNs to
increase the flexibility of distributional regression and mixed-effects 
modelling.
And, Agarwal which drawn on additive structure in GAMs to improve 
interpretability of NNs, in what they term NAMs.

---

# Background

Renewed interest in merging statistical models and neural networks.

--


From a statistical viewpoint:

--


-  Distributional regression (Rugamer et al., 2020, 2021).

--

-  Mixed modelling (Tran et al., 2020).

--

From a machine-learning viewpoint:

--

- Neural Additive Models (Agarwal et al., 2020)


---

# Feedforward Neural Networks 

--

.pull-left[ 
```{r, echo=FALSE, out.width="90%", out.height="110%", fig.align="center"}
knitr::include_graphics("img/FNN.png")
``` 
]

<br>

<br>

???

FNN with single hidden layer.

Neural networks are often represented diagrammatically, so here we have a visual representation of a Feedforward neural network.

They are made up of three different layers.

--

$$
\begin{equation}
\text{NN}(x_i) = \gamma_0+\sum_{k=1}^q \gamma_k \phi \left( \sum_{j=0}^p \omega_{jk}x_{ji}\right)
\end{equation}
$$

???

Of course, we can also represent this neural network with an equation, which is given here.

Explanation

Viewed as a non-linear regression model, where the weights are the parameters of the model, and again the input nodes represent covariates and the hidden layer controls complexity.


---

# Motivating Example

--

### Boston Housing Data (Kaggle)

--

506 communities in Boston, MA.    

--

 
Response: 

- `medv` (median value of owner-occupied homes)    

--

12 Explanatory Variables:  

- `rm` (average number of rooms per dwelling)  

- `lstat` (proportion of population that are disadvantaged)


---

# R Implementation: nnet

--

```{r nnet, echo = TRUE, eval = FALSE}
library(nnet)
nn <- nnet(medv ~ ., data = Boston, size = 8, maxit = 5000,
           linout = TRUE)
summary(nn)
```

--

```{r nnetmodel, eval = TRUE, echo = FALSE, class.output = "bg-primary"}
library(nnet)
set.seed(1010101)
nnet_model <- nnet(medv ~ ., data = Boston, size = 8, maxit = 5000,
                   linout = TRUE, trace = FALSE)
cat(c(capture.output(summary(nnet_model))[c(3:10)], "[...]"),
    sep = "\n") 
```

???

R packages: neuralnet, ann, nnet, keras.

Nnet has history in R, and it is very easy to use.

Useful for prediction. However, it is not very insightful. Looking at the output of the summary for an nnet object, we just get a list of coefficients. 

Unlike outputs from other statistical models, which provides us summary tables containing effects, and p-values. 

---
class: inverse middle center subsection
# Statistical Perspective

---

# Statistical Perspective

--

$$
y_i = \text{NN}(x_i) + \varepsilon_i, 
$$

--

where

$$
\varepsilon_i \sim N(0, \sigma^2)
$$

<br>

--

$$
\ell(\theta)= -\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\text{NN}(x_i))^2
$$


---

# Uncertainty Quantification

Then, as $n \to \infty$

$$
\hat{\theta} \sim N[\theta, \Sigma = \mathcal{I}(\theta)^{-1}]
$$

???

Then, the asymptotic results from maximum likelihood theory apply, so as n goes to infinity, our estimated weight vector is normally distributed around the true weight vector theta and variance-covariance matrix given by the inverse of the information matrix.
--

Estimate $\Sigma$ using

$$
\hat{\Sigma} = I_o(\hat{\theta})^{-1}
$$

???

Of course, we can estimate sigma using the observed information matrix, which we can easily compute from neural network optimiser like nnet, and this can be used in any uncertainty quantification. So this allows us to perform hypothesis tests, and calculate confidence intervals, which are not usually computed for neural networks.

--

<br>  

However, inverting $I_o(\hat{\theta})$ can be problematic in neural networks.

---

# Redundancy

--

Redundant hidden nodes can lead to issues of unidentifiability for some of the parameters (Fukumizu 1996).

<br>

???

If we have a hidden node in our model which provides no contribution in the estimation of the response this node is redundant. This then leads to an issue of unidenitiability for all the weights that enter that hidden node from the input layer.

--

Redundant hidden nodes $\implies$ Singular information matrix.

<br>

--

Model selection is required.

???

So, while it is common in the implementation of these models to select the number of hidden nodes to be quite large to capture all non-linearities, when taking a statistical standpoint and wanting to quantify any uncertainty in your estimates (or functions thereof), model selection is required.


---
class: inverse middle center subsection
# Model Selection

---

# Model Selection

```{r, echo=FALSE, out.width="65%", fig.align="center"}
knitr::include_graphics("img/FNN-ms.png")
``` 

---
count: false
# Model Selection



```{r, echo=FALSE, out.width="65%", fig.align="center"}
knitr::include_graphics("img/FNN-vs.png")
``` 

---
count: false
# Model Selection



```{r, echo=FALSE, out.width="65%", fig.align="center"}
knitr::include_graphics("img/FNN-vsmc.png")
``` 

---
# Proposed Approach

.pull-left[
```{r, echo=FALSE, out.width="100%", fig.align="center"}
knitr::include_graphics("img/FNN1.png")
``` 
]
--
.pull-right[
Three phases for model selection:

{{content}}
]

--

1. Hidden-node selection
{{content}}



--

2. Input-node selection
{{content}}

--

3. Fine tuning
{{content}}

---
# Proposed Approach

--
.center[
<figcaption>Hidden Node Selection</figcaption>
<img src="img/hidden-node-2.png" height="125px"/>
]
--
.center[
<figcaption>Input Node Selection</figcaption>
<img src="img/input-node-2.png" height="125px"/>
]
--
.center[
<figcaption>Fine Tune</figcaption>
<img src="img/fine-tune-2.png" height="125px"/>
]
---

# Objective Function

--

- Machine Learning:

--

$$
\begin{equation}
\text{Out-of-Sample MSE} = \frac{1}{n_\text{val}}\sum_{i=1}^{n_\text{val}} (y_i - NN(x_i))^2
\end{equation}
$$


--

- Proposed:

--


$$
\begin{equation}
\text{BIC} = -2\ell(\hat{\theta}) + \log(n)(K + 1),
\end{equation}
$$

--

$$
\begin{equation}
K = (p+2)q+1
\end{equation}
$$

---

# Simulation Setup


.pull-left[
<br>


True Model: $p = 3$, $q = 3$

]

---
count: false
# Simulation Setup


.pull-left[
<br>


True Model: $p = 3$, $q = 3$


<br>

No. unimportant inputs: $10$


]



---
count: false
# Simulation Setup


.pull-left[
<br>


True Model: $p = 3$, $q = 3$


<br>

No. unimportant inputs: $10$


<br>

Max no. hidden nodes: $10$

]

--

.pull-right[
```{r, echo=FALSE, out.width="90%", fig.align="center"}
knitr::include_graphics("img/simFNN.png")
``` 
]

---

# Simulation Results: Approach

--

```{r, echo=FALSE, out.width="65%", fig.align="center"}
knitr::include_graphics("img/table-sim-approach.png")
``` 

---
# Simulation Results: Objective Function

--

```{r, echo=FALSE, out.width="50%", fig.align="center"}
knitr::include_graphics("img/table-sim-objfun.png")
``` 

--
```{r, echo=FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics("img/table-sim-metrics.png")
```

---
class: inverse middle center subsection
# Statistical Interpretaion


---

# Hypothesis Testing

--

.pull-left[
```{r, echo=FALSE, out.width="100%", fig.align="center"}
knitr::include_graphics("img/FNN1.png")
``` 
]

---
count: false
# Hypothesis Testing


.pull-left[
```{r, echo=FALSE, out.width="100%", fig.align="center"}
knitr::include_graphics("img/FNN2.png")
``` 
]

--

.pull-right[
  Wald test:
  
  {{content}}
  
  ]

--

$$
\begin{equation}
 \omega_j = (\omega_{j1},\omega_{j2},\dotsc,\omega_{jq})^T
\end{equation}
$$
{{content}}

--

$$
\begin{equation}
 H_0: \omega_j = 0
\end{equation}
$$
{{content}}

--

$$
\begin{equation}
 (\hat{\omega}_{j} - \omega_j)^T\Sigma_{\hat{\omega}_{j}}^{-1}(\hat{\omega}_{j} - \omega_j) \sim \chi^2_q
\end{equation}
$$
{{content}}


---

# Simple Covariate Effect

<br>

--

$$
\begin{equation}
  \hat{\tau_j} = E[\text{NN}(X)|x_{(j)} > a] - E[\text{NN}(X)|x_{(j)} < a]
\end{equation}
$$

<br>

--


Usually set $a = m_j$, where $m_j$ is the median value of covariate $j$

--

<br>

Associated uncertainty via delta method / bootstrapping

---

# Covariate-Effect Plots


$$
\begin{equation}
 \overline{\text{NN}}_j(x) = \frac{1}{n}\sum_{i=1}^n \text{NN}(x_{(i,1)}, \ldots,x_{(i,j-1)},x, x_{(i,j+1)}, \ldots, x_{(i,p)})
\end{equation}
$$

???

If we define NN j bar of x, which is a conditional average neural network prediction over the data when all the covariates vary except for covariate j which is fixed to x.

--
Propose covariate-effect plots of the following form:

--

$$
\begin{equation}
 \hat{\beta}_j(x,d) = \overline{\text{NN}}_j(x + d) - \overline{\text{NN}}_j(x)
\end{equation}
$$

--

Usually set $d = \text{SD}(x_j)$

--

Associated uncertainty via delta method.


---
class: inverse middle center subsection
# R Implementation


---


# R Implementation

--

.left-column[ 
<br>
```{r, echo=FALSE, out.width="80%", fig.align="center"}
knitr::include_graphics("img/statnnet.png")
``` 
]

--

.right-column[
<br>
<br>
```{r, echo=TRUE, eval = FALSE}
# install.packages("devtools")
library(devtools)
install_github("andrew-mcinerney/statnnet")
``` 
]

???

We have implemented these concept in an R package, statnnet, which is currently available on GitHub, and will be on CRAN soon. This packagecan be used to perform the proposed model selection approach, and also take an nnet object as an input and calculate a more informative summary with p-values and effects. 

---

# Data Application (Revistied)

### Boston Housing Data (Kaggle)

506 communities in Boston, MA.   

--

 
Response: 

- `medv` (median value of owner-occupied homes)    

--

12 Explanatory Variables:  

- `rm` (average number of rooms per dwelling)  

- `lstat` (proportion of population that are disadvantaged)

???

So, to show statnnet in action, I’m going to revisit the data application. As I said earlier, we have 506 observations of communities within Boston, the response is median value of a house, and we are focusing on two of the twelve predictors, average room size, and a measure of disadvantageness. 

---

# Boston Housing: Model Selection 


```{r selection, echo = TRUE, eval = FALSE}
library(statnnet)
nn <- selectnn(medv ~ ., data = Boston, Q = 10,
               n_init = 10, maxit = 5000)
summary(nn)
```

--

```{r summ, echo = FALSE, cache = TRUE, class.output = "bg-primary"}
cat(c(capture.output(summary(nn))[c(1:23)], "[...]"), sep = "\n")  

```

???

We can perform model selection using a function called selectnn(). We supply a formula, and data, along with Q which is the maximum hidden layer size to be considerd. Then, after running model sselection, we can look at a summary. Here, I have a condensed ouput from summary. As you can can, the model selection procedure selected 8 of the 12 covariates and selected the number of hidden nodes to be 4. Then, just focusing on the two covariates, you can see both were selected, and we also report delta BIC as a measure of variable importance. This is calculated by removing the covariate, refitting the model, calculating the BIC and comparing to the model with the covariate included. 

---

# Boston Housing: Model Comparison

```{r, echo=FALSE, out.width="95%", fig.align="center"}
knitr::include_graphics("img/modelcom_boston-1.png")
``` 

---

# Boston Housing: Model Comparison
```{r, echo=FALSE, out.width="95%", fig.align="center"}
knitr::include_graphics("img/modelcomp_boston_zoom-1.png")
``` 


---

# Boston Housing: Model Summary 


```{r stnn, message = FALSE, eval = FALSE, tidy = FALSE}
stnn <- statnnet(nn)  
summary(stnn)
```

--

```{r summst, echo = FALSE, cache = TRUE, class.output = "bg-primary"}
cat(c("[...]", capture.output(summary(stnn))[12:24]), sep = "\n")  
```

???

We also have a statnnet function, which can take a neural network as input and calculated a more statistically-focused summary table. Here, we have a summary table for our neural network. We have point estimates and their standard error. I didn’t touch on how we calculate these in this talk but there are calculated by splitting the covariate into two ground based on the median value and looking at the the difference in the average prediction of both groups. We also get the Wald test results. So we can see here that rm and lstat are both statistically significant.
---

# Boston Housing: Simple Effects

```{r, echo=FALSE, out.width="90%", fig.align="center"}
knitr::include_graphics("img/BostonEffects1-1.png")
``` 

---

# Boston Housing: Covariate-Effect Plots 

```{r, eval = FALSE}
plot(stnn, conf_int = TRUE, method = "deltamethod", which = c(4, 8))
```  

--

.pull-left[ 
```{r plot1, cache = TRUE, echo = FALSE, fig.height = 6}
par(mar=c(5,6,4,2)+.1)
plot(stnn, conf_int = TRUE, method = "deltamethod", x_axis_r = c(0, 1), 
       which = 4, cex.caption = 2, cex.lab = 1.75, cex.axis = 1.5, caption = "")
# axis(1, c(0, 0.2, 0.4, 0.6, 0.8, 1), labels = 3.5:8.5, pos = -0.035)
axis(3, seq(0, 1, length.out = 5), labels = seq(3.5, 8.5, length.out = 5), 
     cex.axis = 1.5)
```  
]  

--

.pull-right[ 
```{r plot2, cache = TRUE, echo = FALSE, fig.height = 6}
par(mar=c(5,6,4,2)+.1)
plot(stnn, conf_int = TRUE, method = "deltamethod", x_axis_r = c(0, 1), 
     which = 8, cex.caption = 2, cex.lab = 1.75, cex.axis = 1.5, caption = "")
# axis(1, seq(0, 1, length.out = 5), labels = seq(2, 38, length.out = 5),
#      pos = -0.155)
axis(3, seq(0, 1, length.out = 5), labels = seq(2, 38, length.out = 5), 
     cex.axis = 1.5)
```  
] 

???

And, we can also calculate the covariate-effect plots I mentioned earlier, and their associated uncertainty. Here is the code to plot rm and lstat which are the 4th and 8th covariate and their uncertainty which is estimated using the delta method. Here is the rm plot, so you can look at these plots as a varying effect plots. So we can see the effect of average number of rooms on median house price is zero when the houses are quite small, so about 3.5 rooms. But, as room number of rooms increase, the effect gets stronger, and it is positive so it increases the median house value. Then maybe it looks like when we get to an average number of rooms of just over size, the effect seems to become constant of about 0.12. For the lstat plot, we can see that when lstat is low, an increase in lstat is associated with a negative effect of about -0.1, but as lstat increase the effect becomes weaker, and then, for a value of about 29%, increasing lstat has no effect on the value of homes.

---

# Summary 


Feedforward neural networks are non-linear regression models.  

--

Calculation of a likelihood function allows for uncertainty quantification.

--

Statistically-based model selection is required to avoid issues of
unidentifiability.

--

Our R package extends existing neural network packages
to allow for model selection and a more interpretable, statistically-based output.


---

# References
<font size="4">
R. Agarwal, N. Frosst, X. Zhang, R. Caruana and G. E. Hinton, "Neural additive models: Interpretable machine learning with neural nets," arXiv preprint arXiv:2004.13912, 2020.
</font>   
<br>
<font size="4">
K. Fukumizu, 
"A regularity condition of the information  
matrix of a multilayer perceptron network". Neural Networks,  
9(5):871–879. 
</font>
<br>
<font size="4">
D. Rügamer, C. Kolb and N. Klein, "Semi-Structured Deep Distributional Regression: Combining Structured Additive Models and Deep Learning," arXiv preprint arXiv:2002.05777, 2020.
</font>
<br>
<font size="4">
D. Rügamer, C. Kolb, C. Fritz, F. Pfisterer, B. Bischl, R. Shen, C. Bukas, L. Barros de Andrade e Sousa, D. Thalmeier, P. Baumann, N. Klein and C. L. Müller, "deepregression: a Flexible Neural Network Framework for Semi-Structured Deep Distributional Regression," arXiv preprint arXiv:2104.02705, 2021.
</font>
<br>
<font size="4">
M.-N. Tran, N. Nguyen, D. Nott and R. Kohn, "Bayesian deep net GLM and GLMM," Journal of Computational and Graphical Statistics, vol. 29, p. 97–113, 2020.
</font>

---

class: final-slide
# References


McInerney, A. and Burke, K. (2022).
A Statistically-Based Approach  
to Feedforward Neural Network Model Selection.
arXiv preprint arXiv:2207.04248.


### R Package  

```{r, eval = FALSE}
devtools::install_github("andrew-mcinerney/statnnet")
```

<br>

`r fa(name = "github", fill = "#007DBA")` <font size="5">andrew-mcinerney</font>   `r fa(name = "twitter", fill = "#007DBA")` <font size="5">@amcinerney_</font> `r fa(name = "envelope", fill = "#007DBA")` <font size="5">andrew.mcinerney@ul.ie</font>



